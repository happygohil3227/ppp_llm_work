{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cd680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4034138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.amazon_ocr import TextractPDFProcessor\n",
    "\n",
    "# # 1. Initialize processor\n",
    "# processor = TextractPDFProcessor(\n",
    "#     region_name=\"ap-south-1\",\n",
    "#     poll_interval=5\n",
    "# )\n",
    "\n",
    "# # 2. Define S3 location of MCA PDF\n",
    "# bucket_name = \"textract-input-happy\"\n",
    "# object_key = \"CONCOR Agreement for operations_compressed.pdf\"\n",
    "\n",
    "# # 3. Process PDF\n",
    "# pagewise_output = processor.process_pdf(\n",
    "#     bucket=bucket_name,\n",
    "#     key=object_key\n",
    "# )\n",
    "\n",
    "# # 4. Use the output (example)\n",
    "# for page_no, lines in pagewise_output.items():\n",
    "#     print(f\"\\n===== PAGE {page_no} =====\")\n",
    "#     for line in lines:\n",
    "#         print(line[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16217551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "def pdf_to_images(pdf_path):\n",
    "    images = convert_from_path(pdf_path, dpi=300)\n",
    "    return images\n",
    "\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(img: Image.Image):\n",
    "    img = np.array(img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    return gray\n",
    "\n",
    "def ocr_image(img):\n",
    "    config = \"--oem 3 --psm 6\"\n",
    "    text = pytesseract.image_to_string(img, config=config)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_scanned_pdf(pdf_path):\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        processed = preprocess_image(img)\n",
    "        page_text = ocr_image(processed)\n",
    "        full_text += f\"\\n\\n--- Page {i+1} ---\\n\"\n",
    "        full_text += page_text\n",
    "\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2dc9cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "File \u001b[0;32m~/Desktop/iimmu/ppp_llm_work/cag_env310/lib/python3.10/site-packages/pdf2image/pdf2image.py:581\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n",
      "\u001b[1;32m    580\u001b[0m     env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poppler_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m--> 581\u001b[0m proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.19_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n",
      "\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n",
      "\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.19_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n",
      "\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n",
      "\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n",
      "\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pdfinfo'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONCOR Agreement for operations_compressed.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m----> 2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_scanned_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;32m      5\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mextract_text_from_scanned_pdf\u001b[0;34m(pdf_path)\u001b[0m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_text_from_scanned_pdf\u001b[39m(pdf_path):\n",
      "\u001b[0;32m---> 24\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_to_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     25\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images):\n",
      "\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mpdf_to_images\u001b[0;34m(pdf_path)\u001b[0m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpdf_to_images\u001b[39m(pdf_path):\n",
      "\u001b[0;32m----> 4\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "\n",
      "File \u001b[0;32m~/Desktop/iimmu/ppp_llm_work/cag_env310/lib/python3.10/site-packages/pdf2image/pdf2image.py:127\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n",
      "\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(poppler_path, PurePath):\n",
      "\u001b[1;32m    125\u001b[0m     poppler_path \u001b[38;5;241m=\u001b[39m poppler_path\u001b[38;5;241m.\u001b[39mas_posix()\n",
      "\u001b[0;32m--> 127\u001b[0m page_count \u001b[38;5;241m=\u001b[39m \u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mownerpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoppler_path\u001b[49m\n",
      "\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[39;00m\n",
      "\u001b[1;32m    132\u001b[0m parsed_fmt, final_extension, parse_buffer_func, use_pdfcairo_format \u001b[38;5;241m=\u001b[39m _parse_format(\n",
      "\u001b[1;32m    133\u001b[0m     fmt, grayscale\n",
      "\u001b[1;32m    134\u001b[0m )\n",
      "\n",
      "File \u001b[0;32m~/Desktop/iimmu/ppp_llm_work/cag_env310/lib/python3.10/site-packages/pdf2image/pdf2image.py:607\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n",
      "\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFInfoNotInstalledError(\n",
      "\u001b[1;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    609\u001b[0m     )\n",
      "\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFPageCountError(\n",
      "\u001b[1;32m    612\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    613\u001b[0m     )\n",
      "\n",
      "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    }
   ],
   "source": [
    "pdf_path = \"CONCOR Agreement for operations_compressed.pdf\"\n",
    "text = extract_text_from_scanned_pdf(pdf_path)\n",
    "\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"OCR completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46f80d",
   "metadata": {},
   "source": [
    "# high 2026 OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba8e33",
   "metadata": {},
   "source": [
    "### Run below cell to install required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch\n",
    "# !pip install pdf2image pytesseract opencv-python numpy pillow openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract, cv2, numpy as np\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# ---------------- PDF to Images ----------------\n",
    "def pdf_to_images(pdf_path):\n",
    "    return convert_from_path(pdf_path, dpi=300)\n",
    "\n",
    "# ---------------- Image Preprocessing ----------------\n",
    "def preprocess_image(img: Image.Image):\n",
    "    img = np.array(img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    return gray\n",
    "\n",
    "# ---------------- Tesseract OCR ----------------\n",
    "def ocr_image(img):\n",
    "    config = \"--oem 3 --psm 6\"\n",
    "    return pytesseract.image_to_string(img, config=config)\n",
    "\n",
    "# ---------------- GPT Cleanup ----------------\n",
    "def gpt_clean_page(text):\n",
    "    prompt = f\"\"\"\n",
    "You are cleaning OCR errors in a scanned legal document.\n",
    "Fix spelling, broken words, wrong line breaks.\n",
    "Do NOT change meaning or delete clauses.\n",
    "only output the cleaned text.\n",
    "no additional commentary.\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ---------------- Full Pipeline ----------------\n",
    "def extract_text_from_scanned_pdf(pdf_path):\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    full_text = \"\"\"\"\"\"\n",
    "\n",
    "    for i, img in enumerate(tqdm(images, desc=\"Processing pages\")):\n",
    "        processed = preprocess_image(img)\n",
    "        raw_text = ocr_image(processed)\n",
    "        clean_text = gpt_clean_page(raw_text)\n",
    "\n",
    "        full_text += f\"\\n\\n--- Page {i+1} ---\\n\"\n",
    "        full_text += clean_text\n",
    "\n",
    "    return full_text\n",
    "\n",
    "# # ---------------- Run ----------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     pdf_path = \"CONCOR Agreement for operations_compressed.pdf\"\n",
    "\n",
    "#     final_text = extract_text_from_scanned_pdf(pdf_path)\n",
    "\n",
    "#     with open(\"output_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(final_text)\n",
    "\n",
    "#     print(\"OCR + GPT cleanup completed to output_clean.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90ed4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|██████████| 35/35 [01:02<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract, cv2, numpy as np\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# client = OpenAI()\n",
    "client = Groq(api_key=\"groq_api_key\")\n",
    "\n",
    "# ---------------- PDF to Images ----------------\n",
    "def pdf_to_images(pdf_path):\n",
    "    return convert_from_path(pdf_path, dpi=300)\n",
    "\n",
    "# ---------------- Image Preprocessing ----------------\n",
    "def preprocess_image(img: Image.Image):\n",
    "    img = np.array(img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    return gray\n",
    "\n",
    "# ---------------- Tesseract OCR ----------------\n",
    "def ocr_image(img):\n",
    "    config = \"--oem 3 --psm 6\"\n",
    "    return pytesseract.image_to_string(img, config=config)\n",
    "\n",
    "# ---------------- GPT Cleanup ----------------\n",
    "def gpt_clean_page(text):\n",
    "    prompt = f\"\"\"\n",
    "You are cleaning OCR errors in a scanned legal document.\n",
    "Fix spelling, broken words, wrong line breaks.\n",
    "Do NOT change meaning or delete clauses.\n",
    "only output the cleaned text.\n",
    "no additional commentary.\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        # model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ---------------- Full Pipeline ----------------\n",
    "def extract_text_from_scanned_pdf(pdf_path):\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    for i, img in enumerate(tqdm(images, desc=\"Processing pages\")):\n",
    "        processed = preprocess_image(img)\n",
    "        raw_text = ocr_image(processed)\n",
    "        # clean_text = gpt_clean_page(raw_text)\n",
    "\n",
    "        full_text += f\"\\n\\n--- Page {i+1} ---\\n\"\n",
    "        full_text += raw_text\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"GCT Pathri_Concor_compressed.pdf\"\n",
    "\n",
    "    final_text = extract_text_from_scanned_pdf(pdf_path)\n",
    "    # cleaned_text = gpt_clean_page(final_text)\n",
    "\n",
    "    # with open(\"output_clean_fullClean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(final_text)\n",
    "\n",
    "    # print(\"OCR + GPT cleanup completed to output_clean.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b1b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning chunks: 100%|██████████| 4/4 [03:17<00:00, 49.39s/it]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def chunk_text(text, max_tokens=2000, model=\"gpt-4\"):\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # For models not recognized by tiktoken (like Groq's Llama), use a standard encoding\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    tokens = enc.encode(text)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i+max_tokens]\n",
    "        chunks.append(enc.decode(chunk_tokens))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def gpt_clean_chunks(text, model=\"gpt-4\"):\n",
    "    chunks = chunk_text(text, max_tokens=2000, model=model)\n",
    "    cleaned_chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Cleaning chunks\")):\n",
    "        prompt = f\"\"\"\n",
    "You are cleaning OCR output from a scanned legal agreement (Master Concession Agreement).\n",
    "\n",
    "THIS IS NOT A SUMMARY OR REWRITE TASK.\n",
    "\n",
    "IMPORTANT CONTEXT:\n",
    "- The input text MAY BE INCOMPLETE.\n",
    "- The text may start or end mid-sentence or mid-clause.\n",
    "- The text may contain OCR-induced structural errors.\n",
    "\n",
    "STRUCTURAL WARNING (CRITICAL):\n",
    "- ARTICLE numbers or clause numbers may be missing, incorrect, or inconsistent due to OCR.\n",
    "- Do NOT renumber, reorder, normalize, or \"correct\" ARTICLE or clause numbering.\n",
    "- Preserve numbering EXACTLY as it appears in the input.\n",
    "- Do NOT infer the correct ARTICLE number even if it seems obvious.\n",
    "\n",
    "EXAMPLE (DO NOT FIX STRUCTURE):\n",
    "If the input contains:\n",
    "\"ARTICLES\n",
    "SCOPE OF CONCESSION\n",
    "\n",
    "4.1 Concession\n",
    "...\n",
    "3.1.3 The right to operate...\n",
    "...\n",
    "44.4 Rate of Rail Terminal\"\n",
    "\n",
    "Even if it appears that:\n",
    "- This section should be \"ARTICLE 3 – SCOPE OF CONCESSION\"\n",
    "- \"44.4\" likely means \"4.4\" or \"3.4.4\"\n",
    "\n",
    "YOU MUST:\n",
    "- Keep \"ARTICLES SCOPE OF CONCESSION\" exactly as written\n",
    "- Keep clause numbers (4.1, 3.1.3, 44.4) unchanged\n",
    "- Only clean spelling, broken words, and incorrect line breaks\n",
    "\n",
    "YOU MUST NOT:\n",
    "- Insert a missing ARTICLE number\n",
    "- Renumber clauses\n",
    "- Correct clause sequencing\n",
    "- Replace \"44.4\" with a guessed number\n",
    "\n",
    "\n",
    "PRIMARY GOAL:\n",
    "Correct OCR noise while preserving the legal structure and meaning EXACTLY as provided.\n",
    "\n",
    "DOCUMENT STRUCTURE:\n",
    "- ARTICLE headings appear as: \"ARTICLE 1\", \"ARTICLE 2\", etc.\n",
    "- Clauses are numbered hierarchically: 1.1, 1.1.1, 4.6.2, etc.\n",
    "- Clause numbers and ARTICLE headings are authoritative anchors.\n",
    "\n",
    "STRICT RULES (NON-NEGOTIABLE):\n",
    "- Do NOT add, infer, reconstruct, or complete missing text.\n",
    "- Do NOT guess how a sentence, clause, or ARTICLE continues.\n",
    "- Do NOT merge content across clause boundaries.\n",
    "- Do NOT split clauses into new ones.\n",
    "- Do NOT change clause numbering or ordering.\n",
    "- Do NOT modify defined terms or legal capitalization.\n",
    "\n",
    "ALLOWED CLEANING ACTIONS ONLY:\n",
    "- Fix OCR spelling errors.\n",
    "- Join words broken due to line breaks.\n",
    "- Remove incorrect line breaks inside the SAME sentence or clause.\n",
    "- Preserve paragraph breaks BETWEEN clauses.\n",
    "- Leave truncated sentences AS-IS without attempting to complete them.\n",
    "- Remove special tokens or annotations.\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "- Output ONLY the cleaned legal text.\n",
    "- Preserve ARTICLE headings and clause numbers exactly.\n",
    "- Do NOT include explanations, comments, or formatting markers.\n",
    "\n",
    "\n",
    "TEXT TO CLEAN:\n",
    "{chunk}\n",
    "\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        cleaned_chunks.append(response.choices[0].message.content)\n",
    "\n",
    "    return \"\\n\".join(cleaned_chunks)\n",
    "\n",
    "cleaned_text = gpt_clean_chunks(final_text, model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e72b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OCR Pages: 100%|██████████| 35/35 [01:04<00:00,  1.83s/it]\n",
      "Cleaning Chunks: 100%|██████████| 10/10 [04:14<00:00, 25.46s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils.ocr_clean import OCRCleaner\n",
    "\n",
    "ocr = OCRCleaner(\n",
    "    groq_api_key=\"groq_key\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "final_text = ocr.run(\"mca_pdf/GCT Pathri_Concor_compressed.pdf\")\n",
    "\n",
    "with open(\"output_clean__exp1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458258b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0175c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_clean_chunked.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754701ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read output_clean_chunked.txt\n",
    "with open(\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6097809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_text = gpt_clean_chunks(final_text, model=\"openai-gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAUSE_BOUNDARY_PROMPT = \"\"\"\n",
    "You are a legal clause boundary detection engine.\n",
    "\n",
    "TASK:\n",
    "Identify clause boundaries in the given legal text.\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "Return STRICTLY valid JSON.\n",
    "No markdown.\n",
    "No explanations.\n",
    "No comments.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{\n",
    "  \"clauses\": [\n",
    "    {\n",
    "      \"clause_number\": \"string\",\n",
    "      \"start_index\": integer,\n",
    "      \"end_index\": integer\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "RULES:\n",
    "- Detect clauses using numbering patterns like:\n",
    "  1\n",
    "  1.1\n",
    "  1.1.1\n",
    "  2\n",
    "  2.3\n",
    "  10\n",
    "- Clause numbering must match EXACTLY as in the text.\n",
    "- start_index and end_index are CHARACTER OFFSETS in the input text.\n",
    "- end_index must be exclusive.\n",
    "- Do NOT extract clause text.\n",
    "- Do NOT infer missing clauses.\n",
    "- Do NOT merge clauses.\n",
    "- Ignore page numbers, headers, footers, watermarks.\n",
    "\n",
    "INPUT TEXT:\n",
    "<<<TEXT>>>\n",
    "\"\"\"\n",
    "\n",
    "def build_clause_boundary_prompt(text: str) -> str:\n",
    "    return CLAUSE_BOUNDARY_PROMPT.replace(\"<<<TEXT>>>\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c767e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def token_len(text, model=\"cl100k_base\"):\n",
    "    enc = tiktoken.get_encoding(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def chunk_with_overlap(text, max_tokens=3000, overlap_tokens=200):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(text)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = start + max_tokens\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunks.append(enc.decode(chunk_tokens))\n",
    "        start = end - overlap_tokens\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def deduplicate_and_sort_clauses(clauses):\n",
    "    seen = {}\n",
    "    for c in clauses:\n",
    "        key = (c[\"clause_number\"], c[\"start_index\"])\n",
    "        if key not in seen:\n",
    "            seen[key] = c\n",
    "\n",
    "    result = list(seen.values())\n",
    "\n",
    "    result.sort(\n",
    "        key=lambda x: [int(p) if p.isdigit() else p for p in x[\"clause_number\"].split(\".\")]\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_json_from_response(text):\n",
    "    \"\"\"Extract JSON from LLM response, handling markdown code blocks.\"\"\"\n",
    "    # Try to find JSON in markdown code blocks first\n",
    "    json_match = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(1)\n",
    "    \n",
    "    # Try to find raw JSON object\n",
    "    json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json_match.group(0)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def detect_clause_boundaries(text, client, model):\n",
    "    chunks = chunk_with_overlap(text)\n",
    "    all_clauses = []\n",
    "\n",
    "    for i, chunk in enumerate(tqdm(chunks,desc=\"processing the chunks in document\")):\n",
    "        prompt = build_clause_boundary_prompt(chunk)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        try:\n",
    "            # Try to extract JSON from response\n",
    "            json_str = extract_json_from_response(response_text)\n",
    "            chunk_result = json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Could not parse JSON from chunk {i}. Error: {str(e)[:100]}\")\n",
    "            print(f\"Response preview: {response_text[:200]}\")\n",
    "            continue\n",
    "\n",
    "        if \"clauses\" in chunk_result:\n",
    "            for c in chunk_result[\"clauses\"]:\n",
    "                all_clauses.append(c)\n",
    "\n",
    "    return deduplicate_and_sort_clauses(all_clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5286057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not parse JSON from chunk 3. Error: Expecting ',' delimiter: line 1412 column 6 (char 25641)\n",
      "Response preview: {\n",
      "  \"clauses\": [\n",
      "    {\n",
      "      \"clause_number\": \"1\",\n",
      "      \"start_index\": 0,\n",
      "      \"end_index\": 5\n",
      "    },\n",
      "    {\n",
      "      \"clause_number\": \"1\",\n",
      "      \"start_index\": 6,\n",
      "      \"end_index\": 12\n",
      "    },\n",
      "    {\n",
      "    \n",
      "Warning: Could not parse JSON from chunk 4. Error: Expecting ',' delimiter: line 1317 column 6 (char 24961)\n",
      "Response preview: {\n",
      "  \"clauses\": [\n",
      "    {\n",
      "      \"clause_number\": \"1\",\n",
      "      \"start_index\": 0,\n",
      "      \"end_index\": 5\n",
      "    },\n",
      "    {\n",
      "      \"clause_number\": \"1.1\",\n",
      "      \"start_index\": 6,\n",
      "      \"end_index\": 11\n",
      "    },\n",
      "    {\n",
      "  \n",
      "Warning: Could not parse JSON from chunk 5. Error: Expecting ',' delimiter: line 1412 column 6 (char 26098)\n",
      "Response preview: {\n",
      "  \"clauses\": [\n",
      "    {\n",
      "      \"clause_number\": \"1\",\n",
      "      \"start_index\": 0,\n",
      "      \"end_index\": 6\n",
      "    },\n",
      "    {\n",
      "      \"clause_number\": \"2\",\n",
      "      \"start_index\": 7,\n",
      "      \"end_index\": 13\n",
      "    },\n",
      "    {\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "final_text01 = detect_clause_boundaries(cleaned_text, client, model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92909e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'argo '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_text[1291:1296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64c330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clause_number': '1', 'start_index': 0, 'end_index': 5},\n",
       " {'clause_number': '2', 'start_index': 6, 'end_index': 13},\n",
       " {'clause_number': '3', 'start_index': 246, 'end_index': 249},\n",
       " {'clause_number': '3.1', 'start_index': 1045, 'end_index': 1050},\n",
       " {'clause_number': '4', 'start_index': 1101, 'end_index': 1104},\n",
       " {'clause_number': '4.1', 'start_index': 1105, 'end_index': 1110},\n",
       " {'clause_number': '4.1.1', 'start_index': 1121, 'end_index': 1126},\n",
       " {'clause_number': '4.1.2', 'start_index': 1131, 'end_index': 1136},\n",
       " {'clause_number': '4.1.3', 'start_index': 1141, 'end_index': 1146},\n",
       " {'clause_number': '4.2', 'start_index': 1151, 'end_index': 1156},\n",
       " {'clause_number': '4.2.1', 'start_index': 1163, 'end_index': 1168},\n",
       " {'clause_number': '4.2.24', 'start_index': 1231, 'end_index': 1236},\n",
       " {'clause_number': '4.3', 'start_index': 1241, 'end_index': 1246},\n",
       " {'clause_number': '4.5', 'start_index': 1261, 'end_index': 1266},\n",
       " {'clause_number': '4.5.1', 'start_index': 1281, 'end_index': 1286},\n",
       " {'clause_number': '4.5.2', 'start_index': 1291, 'end_index': 1296},\n",
       " {'clause_number': '4.5.2.1', 'start_index': 1305, 'end_index': 1310},\n",
       " {'clause_number': '4.5.2.2', 'start_index': 1315, 'end_index': 1320},\n",
       " {'clause_number': '4.5.2.4', 'start_index': 1331, 'end_index': 1336},\n",
       " {'clause_number': '4.5.2.5', 'start_index': 1341, 'end_index': 1346},\n",
       " {'clause_number': '5', 'start_index': 1351, 'end_index': 1356},\n",
       " {'clause_number': '5.5.2', 'start_index': 1361, 'end_index': 1366},\n",
       " {'clause_number': '5.5.2', 'start_index': 1371, 'end_index': 1376},\n",
       " {'clause_number': '6', 'start_index': 1381, 'end_index': 1386},\n",
       " {'clause_number': '6', 'start_index': 0, 'end_index': 6},\n",
       " {'clause_number': '6.1', 'start_index': 1391, 'end_index': 1396},\n",
       " {'clause_number': '6.1', 'start_index': 7, 'end_index': 15},\n",
       " {'clause_number': '6.1.1', 'start_index': 1401, 'end_index': 1406},\n",
       " {'clause_number': '6.1.1', 'start_index': 16, 'end_index': 30},\n",
       " {'clause_number': '7', 'start_index': 31, 'end_index': 38},\n",
       " {'clause_number': '7.3', 'start_index': 39, 'end_index': 46},\n",
       " {'clause_number': '7.3.1', 'start_index': 47, 'end_index': 61},\n",
       " {'clause_number': '7.3.2', 'start_index': 62, 'end_index': 76},\n",
       " {'clause_number': '7.3.3', 'start_index': 77, 'end_index': 91},\n",
       " {'clause_number': '7.3.4', 'start_index': 92, 'end_index': 106},\n",
       " {'clause_number': '7.3.4.1', 'start_index': 107, 'end_index': 122},\n",
       " {'clause_number': '7.3.4.2', 'start_index': 123, 'end_index': 138},\n",
       " {'clause_number': '7.3.5', 'start_index': 139, 'end_index': 154},\n",
       " {'clause_number': '7.3.6', 'start_index': 155, 'end_index': 170},\n",
       " {'clause_number': '7.4', 'start_index': 171, 'end_index': 178},\n",
       " {'clause_number': '7.5', 'start_index': 179, 'end_index': 186},\n",
       " {'clause_number': '8', 'start_index': 187, 'end_index': 194},\n",
       " {'clause_number': '8.1', 'start_index': 195, 'end_index': 210},\n",
       " {'clause_number': '8.2', 'start_index': 211, 'end_index': 226},\n",
       " {'clause_number': '9', 'start_index': 227, 'end_index': 234},\n",
       " {'clause_number': '9.1', 'start_index': 235, 'end_index': 250},\n",
       " {'clause_number': '9.2', 'start_index': 251, 'end_index': 266},\n",
       " {'clause_number': '9.3', 'start_index': 267, 'end_index': 282},\n",
       " {'clause_number': '10', 'start_index': 283, 'end_index': 290},\n",
       " {'clause_number': '10.1', 'start_index': 291, 'end_index': 306},\n",
       " {'clause_number': '10.2', 'start_index': 307, 'end_index': 322},\n",
       " {'clause_number': '10.3', 'start_index': 323, 'end_index': 338},\n",
       " {'clause_number': '11', 'start_index': 339, 'end_index': 346},\n",
       " {'clause_number': '11.1', 'start_index': 347, 'end_index': 362},\n",
       " {'clause_number': '11.2', 'start_index': 363, 'end_index': 378},\n",
       " {'clause_number': '12', 'start_index': 379, 'end_index': 386},\n",
       " {'clause_number': '13', 'start_index': 387, 'end_index': 394},\n",
       " {'clause_number': '13.1', 'start_index': 395, 'end_index': 410},\n",
       " {'clause_number': '13.2', 'start_index': 411, 'end_index': 426},\n",
       " {'clause_number': '13.2.1', 'start_index': 427, 'end_index': 442},\n",
       " {'clause_number': '13.2.2', 'start_index': 443, 'end_index': 458},\n",
       " {'clause_number': '14', 'start_index': 459, 'end_index': 466},\n",
       " {'clause_number': '14.1', 'start_index': 467, 'end_index': 482},\n",
       " {'clause_number': '15', 'start_index': 483, 'end_index': 490},\n",
       " {'clause_number': '15.1', 'start_index': 491, 'end_index': 506},\n",
       " {'clause_number': '15.1.2', 'start_index': 507, 'end_index': 522},\n",
       " {'clause_number': '15.2', 'start_index': 523, 'end_index': 538},\n",
       " {'clause_number': '15.3.1', 'start_index': 539, 'end_index': 554},\n",
       " {'clause_number': '15.9', 'start_index': 555, 'end_index': 570},\n",
       " {'clause_number': '15.10', 'start_index': 571, 'end_index': 586},\n",
       " {'clause_number': '15.11', 'start_index': 587, 'end_index': 602},\n",
       " {'clause_number': '15.11', 'start_index': 34, 'end_index': 54},\n",
       " {'clause_number': '15.11.1', 'start_index': 603, 'end_index': 618},\n",
       " {'clause_number': '15.11.1', 'start_index': 55, 'end_index': 72},\n",
       " {'clause_number': '15.11.2', 'start_index': 73, 'end_index': 91},\n",
       " {'clause_number': '15.11.3', 'start_index': 92, 'end_index': 112},\n",
       " {'clause_number': '15.12', 'start_index': 113, 'end_index': 136},\n",
       " {'clause_number': '15.12.1', 'start_index': 137, 'end_index': 157},\n",
       " {'clause_number': '15.12.2', 'start_index': 158, 'end_index': 179},\n",
       " {'clause_number': '15.12.3', 'start_index': 180, 'end_index': 201},\n",
       " {'clause_number': '15.12.4', 'start_index': 202, 'end_index': 223},\n",
       " {'clause_number': '15.13', 'start_index': 224, 'end_index': 244},\n",
       " {'clause_number': '15.13.1', 'start_index': 245, 'end_index': 265},\n",
       " {'clause_number': '15.13.2', 'start_index': 266, 'end_index': 286},\n",
       " {'clause_number': '16', 'start_index': 287, 'end_index': 296},\n",
       " {'clause_number': '16.1', 'start_index': 297, 'end_index': 313},\n",
       " {'clause_number': '16.1.1', 'start_index': 314, 'end_index': 332},\n",
       " {'clause_number': '16.1.2', 'start_index': 333, 'end_index': 354},\n",
       " {'clause_number': '17', 'start_index': 355, 'end_index': 364},\n",
       " {'clause_number': '17.2', 'start_index': 365, 'end_index': 384},\n",
       " {'clause_number': '17.3', 'start_index': 385, 'end_index': 405},\n",
       " {'clause_number': '17.3.1', 'start_index': 406, 'end_index': 426},\n",
       " {'clause_number': '17.3.2', 'start_index': 427, 'end_index': 447},\n",
       " {'clause_number': '17.3.3', 'start_index': 448, 'end_index': 469},\n",
       " {'clause_number': '17.4', 'start_index': 470, 'end_index': 481},\n",
       " {'clause_number': '17.5', 'start_index': 482, 'end_index': 494},\n",
       " {'clause_number': '17.5.1', 'start_index': 495, 'end_index': 514},\n",
       " {'clause_number': '17.6', 'start_index': 515, 'end_index': 529},\n",
       " {'clause_number': '17.7', 'start_index': 530, 'end_index': 546},\n",
       " {'clause_number': '17.8', 'start_index': 547, 'end_index': 562},\n",
       " {'clause_number': '17.9', 'start_index': 563, 'end_index': 576},\n",
       " {'clause_number': '17.10', 'start_index': 577, 'end_index': 593},\n",
       " {'clause_number': '17.11', 'start_index': 594, 'end_index': 609},\n",
       " {'clause_number': '17.12', 'start_index': 610, 'end_index': 624},\n",
       " {'clause_number': '17.13', 'start_index': 625, 'end_index': 644},\n",
       " {'clause_number': '17.13.1', 'start_index': 645, 'end_index': 665},\n",
       " {'clause_number': '17.13.2', 'start_index': 666, 'end_index': 686},\n",
       " {'clause_number': '17.13.3', 'start_index': 687, 'end_index': 708},\n",
       " {'clause_number': '17.14', 'start_index': 709, 'end_index': 725}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_text01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"struct_text.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(struct_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd32d7",
   "metadata": {},
   "source": [
    "# .txt to json structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ae5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "import re\n",
    "\n",
    "ARTICLE_PATTERN = re.compile(\n",
    "    r\"(ARTICLE\\s+[IVX0-9#]+)(.*?)((?=ARTICLE\\s+[IVX0-9#]+)|\\Z)\",\n",
    "    re.DOTALL | re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_articles(text):\n",
    "    articles = []\n",
    "    for match in ARTICLE_PATTERN.finditer(text):\n",
    "        article_id = match.group(1).strip()\n",
    "        article_body = match.group(2).strip()\n",
    "        articles.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"raw_text\": article_body\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "def extract_article_title(article_text):\n",
    "    lines = article_text.splitlines()\n",
    "    for line in lines[:5]:\n",
    "        if line.strip().isupper() and len(line.strip()) > 5:\n",
    "            return line.strip()\n",
    "    return None\n",
    "\n",
    "CLAUSE_PATTERN = re.compile(\n",
    "    r\"(?P<id>\\d{1,2}(\\.\\d+)+)\\s+(?P<text>.*?)(?=(\\n\\d{1,2}(\\.\\d+)+\\s)|\\Z)\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def extract_clauses(article_text):\n",
    "    clauses = []\n",
    "    for match in CLAUSE_PATTERN.finditer(article_text):\n",
    "        clauses.append({\n",
    "            \"clause_id\": match.group(\"id\"),\n",
    "            \"raw_text\": match.group(\"text\").strip(),\n",
    "            \"clean_text\": None,\n",
    "            \"confidence\": None\n",
    "        })\n",
    "    return clauses\n",
    "\n",
    "def build_structured_doc(text, document_id):\n",
    "    articles_raw = extract_articles(text)\n",
    "    structured = {\n",
    "        \"document_id\": document_id,\n",
    "        \"articles\": []\n",
    "    }\n",
    "\n",
    "    for art in articles_raw:\n",
    "        title = extract_article_title(art[\"raw_text\"])\n",
    "        clauses = extract_clauses(art[\"raw_text\"])\n",
    "\n",
    "        structured[\"articles\"].append({\n",
    "            \"article_id\": art[\"article_id\"],\n",
    "            \"title\": title,\n",
    "            \"clauses\": clauses\n",
    "        })\n",
    "\n",
    "    return structured\n",
    "\n",
    "import json\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = load_text(\"output_clean.txt\")\n",
    "    structured_doc = build_structured_doc(\n",
    "        text=text,\n",
    "        document_id=\"Rail_MCA_2007\"\n",
    "    )\n",
    "    save_json(structured_doc, \"rail_mca_structured.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287fb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/dt/nn2d_8jn65x8pk2905qm0yfc0000gn/T/ipykernel_1310/2057703706.py\", line 3, in <module>\n",
      "    from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/transformers/__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/transformers/utils/generic.py\", line 51, in <module>\n",
      "    import torch\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/happy/Desktop/iimmu/cag_rake_uti/cag_env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# law-ai/InLegalBERT\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c6425",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_text\u001b[39m(text, model, tokenizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate embedding for a single text using InLegalBERT\"\"\"\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def embed_text(text, model, tokenizer, device=\"cpu\"):\n",
    "    \"\"\"Generate embedding for a single text using InLegalBERT\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Use [CLS] token embedding (first token of last hidden state)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return normalize(embedding)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f16e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/happy/Desktop/iimmu/ppp_llm_work/cag_env310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('law-ai/InLegalBERT')\n",
    "model = AutoModel.from_pretrained('law-ai/InLegalBERT', use_safetensors=True)\n",
    "\n",
    "def embed_text(text, model, tokenizer, device=\"cpu\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\",truncation=True, max_length=512, padding=True)\n",
    "    # inputs= {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(documents, model, tokenizer, device=\"cpu\"):\n",
    "    \"\"\"Embed a list of documents using InLegalBERT\"\"\"\n",
    "    embeddings = []\n",
    "    for doc in documents:\n",
    "        emb = embed_text(doc, model, tokenizer, device)\n",
    "        embeddings.append(emb)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c48caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/happy/Desktop/iimmu/ppp_llm_work/cag_env310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class InLegalBERTEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"law-ai/InLegalBERT\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, use_safetensors=True)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(normalize(embedding)[0])\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# Use it\n",
    "embeddings = InLegalBERTEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16310b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class InLegalBERTEmbeddings(Embeddings):\n",
    "    \"\"\"Custom embeddings using InLegalBERT\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "        self.model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\", use_safetensors=True)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embed multiple documents\"\"\"\n",
    "        embeddings = []\n",
    "        for text in tqdm(texts):\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                padding=True\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            # Convert to numpy safely\n",
    "\n",
    "            # Extract [CLS] token embedding\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().detach().numpy()\n",
    "            embedding = normalize(embedding)[0]  # Shape: (768,)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "            # embedding = outputs.last_hidden_state[:, 0, :].detach()\n",
    "            # embedding = torch.nn.functional.normalize(embedding, dim=1)\n",
    "            # embedding = embedding.cpu()\n",
    "            # embeddings.append(embedding)\n",
    "        \n",
    "        # return embeddings\n",
    "        return np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embed a single query\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# Reinitialize embeddings\n",
    "embeddings = InLegalBERTEmbeddings()\n",
    "print(\"✓ Embeddings loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "txt_path = \"output_clean_chunked.txt\"\n",
    "def load_documents(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def create_vectorDB(file_path, embeddings, db_path=None, chunk_size=1000, overlap=200):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    text = load_documents(file_path)\n",
    "    print(\"Documents loaded.\")\n",
    "    # Chunking\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "    documents = [\n",
    "        Document(page_content=chunk, \n",
    "                metadata={\"source\": os.path.basename(file_path), \"chunk_id\": i})\n",
    "        for i, chunk in enumerate(tqdm(chunks))\n",
    "    ]\n",
    "\n",
    "    print(\"Embedding documents...\")\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    if db_path:\n",
    "        vector_store.save_local(db_path)\n",
    "        print(f\"Vector DB saved to {db_path}\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded.\n",
      "Total chunks created: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 7931.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:37<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB saved to faiss_inlegalbert_db\n"
     ]
    }
   ],
   "source": [
    "vector_store = create_vectorDB(txt_path, embeddings, db_path=\"faiss_inlegalbert_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd844db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding docs: 100%|██████████| 80/80 [00:29<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "} the responsibility of accident/derailment after enquiry is fixed on the\n",
      "GCTO, ART Charges shall be payable by GCTO as prescribed by\n",
      "Railway from time to time. The due Charges will be deducted from\n",
      "} the future payments by Railway to GCTO. However, in case no\n",
      "payment is being made by Railway to GCTO, the GCTO shall pay ART\n",
      "Charges to Railway without\n",
      "con ducting any accident enquiry as mentioned in para 9.2.3 above shall be\n",
      "jointly by the representatives of Railway and GCTO to be\n",
      "the ed by DRM and GCTO respectively. The accepting authority of\n",
      "f enquiry Report shall be DRM, whose decision shall be final and\n",
      "binding on GCTO.\n",
      "10. C&W Maintenance Facilities\n",
      "10.1 Normally C&W facilities shall not be constructed at GCTs.\n",
      "10.2 However, if C&W facilities are operationally required at any GCT\n",
      "as per the extant instructions, only one-time capital cost for setting-up\n",
      "these facilities shall be borne by the GCTO, Operational costs, Including\n",
      "4.2.1 The GCTO owner, in order to expedite commissioning of his terminal, may also opt for bearing the cost of common-user traffic facilities that is to be formally borne by Railways, subject to the condition that:\n",
      "\n",
      "\n",
      "- Work shall be executed through Railways approved consultant/contractors.\n",
      "- Ownership of such assets will remain with Railways.\n",
      "- The detailed estimate shall be prepared and vetted by the Division.\n",
      "\n",
      "\n",
      "4.2.24 An amount equivalent to ten percent (10%) freight discount only on the total goods traffic handled (inward as well as outward) in the GCT shall be reimbursed to the GCTO in lieu of bearing the capital cost for common-user traffic facilities. This reimbursement of capital cost to GCTO shall be done after due verification from the Accounts department of the concerned Division, and shall be for a period of ten years or till the recovery of capital cost, whichever is earlier.\n",
      "4.5.2.2 The repayment of capital cost as mentioned in para 4.5.2.1 above will be done only if the Terminal achieves the target of loading one million Tonne per annum within two Calendar Years of commissioning (excluding the year of Commissioning), For example, if a GCT commissioned during calendar year 2022 achieves one million Tonne loading during calendar year 2024, then the repayment of capital cost of new Block Hut/ Block station (through 10% rebate on outward traffic) shall start from 1st April 2025.\n",
      "\n",
      "\n",
      "4.5.2.4 This provision of repayment shall be applicable only for GCTs for which Agreement between RA and GCTO has not been entered into till the date of issue of this Policy.\n",
      "Where the Applicant is a joint family governed by the Mitakshara School of Hindu Law, \"and A.B. (insert full name) for self and as Karta or managing member of the Joint family, governed by the Mitakshara School of Hindu Law, carrying on business under the name or style of (insert name under which the joint family business is carried on) at (insert address) and C.D. (insert full name) being the other adult members of the said Joint family.\"\n",
      "\n",
      "\n",
      "2. Agreement To Construct Gati Shakti Multi-Modal Cargo Terminal\n",
      "\n",
      "\n",
      "Subject to the terms and conditions hereinafter contained, the Railway Administration shall at the cost and expense of the Applicant In all respects, construct partly or fully on the land of the Railway Administration and partly or fully on the land of the Applicant of the said Cargo Terminal, from (length of) kilometres on (Name of portion) Branch as shown in red and green on the plan annexed hereinafter (bearing CE's No.\n",
      "Legal Representative means any person who is competent to give the Railway Administration a valid discharge in respect of any money or property which may be payable or deliverable to the Applicant and shall include the executor and administrator of a deceased person, a succession certificate Holder, the surviving or continuing partners or members in the case of a firm, association, or body of individuals, the certificated guardian if a minor's property, the committee of lunatic, the assignee or receiver of an insolvent's estate, the liquidator of a Company, a receiver, and any person legally appointed to represent the estate of the Applicant\n",
      "\n",
      "\n",
      "Meaning of other key terms not mentioned above, shall be same as given in para 2 (Definitions) of the Gati Shakti Multi-modal Cargo Terminal Policy issued vide Freight Marketing Master Circular/2022 dated 06.12.2022\n",
      "\n",
      "\n",
      "Forms Of Description Of The Applicant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.vecDb import InLegalMCAStore\n",
    "\n",
    "store = InLegalMCAStore()\n",
    "\n",
    "vector_db = store.build_from_file(\n",
    "    \"output_clean_chunked.txt\",\n",
    "    db_path=\"faiss_mca_db\"\n",
    ")\n",
    "\n",
    "results = vector_db.similarity_search(\n",
    "    \"termination payment on authority default\",\n",
    "    k=5\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(r.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba04c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 178}\n",
      "leviable to all types of container traffic as per extantirules.\n",
      "2.3\n",
      "Empty containers, when moved in privately owned wagons shall be charged at 65% of the\n",
      "rates for loaded single deck 20 Tonnes container rute. The rates are given in Rate Table at\n",
      "Annexure-L\n",
      "2,4\n",
      "In case of Double stack container train operation in privately owned wagons, containers in\n",
      "the upper stuck, whether loaded or empty, shall be charged at 50% of the normal rate and\n",
      "lower stack containers will be charged as per normal tariff.\n",
      "2.5\n",
      "Privately owned empty flat wagons shall be charged at 60% of the rates for\n",
      "loaded single deck 20 Tonnes container rate.\n",
      "2.6\n",
      "There shall be no recovery from operator for maintenance of wagons.\n",
      "&\n",
      "Q\n",
      "2.7\n",
      "All extant Commercial Rules in vogue regarding levy of punitive charges for\n",
      "overloading, penalty for mis-declaration, weighment etc. will be applicable to the\n",
      "container traffic, provided if is not in contravention with any instruction\n",
      "mentioned herein under.\n",
      "2.8\n",
      "The haulage charges for contain\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 177}\n",
      "&\n",
      "of\n",
      "Annexure V\n",
      "GOVERNMENT OF INDIA\n",
      "MINISTRY OF RAILWAYS\n",
      "RAILWAY BOARD\n",
      "No. 2006/TT-HI1/73/12\n",
      "Rail Bhavan, New Delhi,\n",
      "dt.11.10.2006\n",
      "The General Managers,\n",
      "All Indian Railways\n",
      "Sub: Haulage charges recoverable for movement of containers in privately owned\n",
      "wagons.\n",
      "1.0\n",
      "In supersession of letter No.2006/TT-111/73/2 dated 23.3.2006, sanction of Ministry of\n",
      "railways is accorded for levy of handage charges as per the rules and rates prescribed\n",
      "herein under. These charges will be payable by all operators including CONCOR to\n",
      "Indian railway for container traffic.\n",
      "2.0\n",
      "Haulage charges of container trains\n",
      "It has been decided that:-\n",
      "2.1\n",
      "There shall be a single uniform rate for both domestic and EXIM container traffic. The\n",
      "rates are based on the payload of the containers including the weight of containers. The\n",
      "rates are given in Rates Table at Annexure-I\n",
      "2.2\n",
      "The rates for haulage charges mentioned in the Rate Tables are the base rates. Surcharges\n",
      "like busy season surcharge, busy route surcharge etc., wh\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 170}\n",
      "LENDERS' REPRESENTATIVE\n",
      "BY:\n",
      "Name:\n",
      "Title:\n",
      "by\n",
      "of\n",
      "Annexure II\n",
      "Not Used\n",
      "Annexure III\n",
      "GOVERNMENT OF INDIA\n",
      "MINISTRY OF RAILWAYS\n",
      "(RAILWAY BOARD)\n",
      "No.TC-I/2002/214/5\n",
      "New Delhi, dated 22.2.2006\n",
      "General Managers(Comml.)\n",
      "All Indian Railways.\n",
      "Sub: (i) All India Costs for Diesel Shunting Engine Hour and Train Engine Hour\n",
      "for recovery of siding and shunting charges\n",
      "(ii) All India Costs for Electric Train Engine Hour for recovery of siding and\n",
      "shunting charges\n",
      "Ref: Board's letter of even number dated 15.2.2005,\n",
      "Ministry of Railways have decided to revise the All India Costs for Diesel\n",
      "Shunting Engine and Train Engine Hour for Broad Gauge and Meter Gauge and\n",
      "Electric Train Engine Hour for Broad Gauge circulated vide Board's letter cited\n",
      "above, with effect from 01.4.2006(first of April two thousand six) as under:\n",
      "Broad Gauge\n",
      "Meter Gauge\n",
      "Diesel Engine\n",
      "( Cost per hour )\n",
      "(Cost per hour)\n",
      "Shunting Engine\n",
      "Rs.3290/-\n",
      "Rs.4050/-\n",
      "Train Engine\n",
      "Rs.4140/-\n",
      "Rs.5680/-\n",
      "Electric Engine\n",
      "Train Engine\n",
      "Rs.7120/-\n",
      "I\n",
      "2.0\n",
      "Costs t\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 173}\n",
      "I\n",
      "If\n",
      "Annexure-IV\n",
      "Government of India\n",
      "Ministry of Railways\n",
      "Railway Board\n",
      "No.TC-1/98/201/4\n",
      "New Delhi, Dt.18.11.2006\n",
      "General Managers(Commercial)\n",
      "General Managers(Operating)\n",
      "All Indian Railways:\n",
      "Sub: Levy of stabling charge/demarrage charge on privately/jointly owned\n",
      "wagons\n",
      "Ref: Board's letter even number dt.11.8.1999\n",
      "In supersession of all earlier instructions on the subject, it has been decided to revise\n",
      "the instructions regarding levy of stabling charge/demurrage charge on privately/jointly\n",
      "owned wagons. Accordingly, revised guidelines on the subject are as follows:\n",
      "2.0\n",
      "Guidelines for levy of Stabling charge on privately owned stock\n",
      "2.1\n",
      "Stabling charge is levied for detention of privately owned stock at a ruilway premise in any\n",
      "of the following circumstance:\n",
      "when party is unable to receive such stock in their siding\n",
      "when party declines to accept such stock in their siding\n",
      "2.2\n",
      "Privately owned wagons includes wagons procured under \"Own Your Wagon\n",
      "Scheme(Category-C)\",, Defence owned stock\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 179}\n",
      "mentioned herein under.\n",
      "2.8\n",
      "The haulage charges for container trains shall be payable to the Railways for a\n",
      "minimum number of wagons, depending upon the type of stock, as given in the\n",
      "table below. For all purposes, one wagon shall be treated as equivalent to two\n",
      "TEUs. The composition of rake should not be more than standard composition\n",
      "given below:\n",
      "Type of\n",
      "Minimum composition for\n",
      "Standard composition for\n",
      "Stock\n",
      "charge\n",
      "charge\n",
      "BLCA\n",
      "40 wagons= 80 TEUs\n",
      "45 wagons= 90 TEUs\n",
      "BLLA\n",
      "35 wagons= = 70 TEUs\n",
      "40 wagons - 80 TEUs\n",
      "Other\n",
      "30 wagons If 60 TEUs\n",
      "35 wagons= 70 TEUs\n",
      "3.0\n",
      "System for charging\n",
      "3.1\n",
      "The rates for haulage charges for loaded containers (TEUs), Empty containers and Empty\n",
      "flat wagons owned by operators are given in the enclosed Rate Tables at Annexure -1.\n",
      "3.2 The haulage charges will be calculated for\n",
      "(i)The actual number of loaded containers, subject to a minimum of 2 TEUs per wagon.\n",
      "(ii)The actual number of empty containers, subject to a minimum of 2 TEUs per wagon.\n",
      "(iii)\n",
      "The actual num\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 180}\n",
      "(iii)\n",
      "The actual number of empty flat wagons in the rake.\n",
      "3.3 If the total number of wagons in a rake is less than the minimum composition prescribed\n",
      "above, haulage charges for the short fall in the number of wagons shall be calculated at\n",
      "the rates prescribed for empty flat wagons.\n",
      "3.4 To arrive at the total haulage charges for each train, all the haulage charges calculated\n",
      "separately, as per para 3.2 and 3.3 above, will be added together.\n",
      "4.0\n",
      "Hub and Spoke system\n",
      "Operator can operate Hub and Spoke system for transportation of containers in certain\n",
      "regions for which the system of documentation and calculation of haulage charges would\n",
      "be as follows:-\n",
      "4.1 Hub is a container loading facility/depot, which will be used by an operator to\n",
      "aggregate/disseminate traffic. For this purpose the operator may make a request to\n",
      "Railway indicating the pairs of station between which he will run his container trains via\n",
      "8\n",
      "of\n",
      "this hub. Depending on operational feasibility and ensuring that this feasibili\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 83}\n",
      "Railway Administration of the Concessionaire's Trains, this Agreement and any and all\n",
      "services rendered by Railway Administration pursuant hereto. The haulage charges as on the\n",
      "date of execution of this agreement are provided in the Annexure V.\n",
      "10.2. in the event of the load on a wagon exceeding the prescribed limits, the following shall\n",
      "apply:-\n",
      "0 if the overloading doesn't adversely affect sufe operations with suitable speed restrictions,\n",
      "an additional charge (over and above the Haulage Charges) us prescribed by Railway\n",
      "Administration from time to time be payable by the Concessionaire to Railway\n",
      "Administration.\n",
      "ii) if the overloading adversely affects the safe operation, in addition to additional charge\n",
      "(over and above the Haulage Charges) as prescribed by Railway Administration from time to\n",
      "time be payable by the Concessionaire to Railway Administration, the container may be off\n",
      "londed or wagon be detached at the Concessionaire's risk and cost.\n",
      "10.3 Remittance of Haulage Charges\n",
      "10.3\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 65}\n",
      "the Detention Charges to Railway Administration, the Indent for locomotive shall deemed to\n",
      "have impsed, with no further effect.\n",
      "&\n",
      "24\n",
      "6.3.3\n",
      "Locomotive Detention Charges payable by the Concessionaire for detention of a Incomotive\n",
      "beyond the Free Waiting Time shall be at the rates as prescribed for Diesel/Electric Train\n",
      "Engines by the Railway Administration from time to time. The Locomotive Detention\n",
      "Charges as on the date of execution of this agreement are provided in the Annexure-III\n",
      "6.3.4\n",
      "The Parties agree that the provisions of Article 6.3.2 shall apply mutoris mutandis to the\n",
      "situation where the locomotive arrives at a Rail Terminal where the train is scheduled to\n",
      "terminate, but is denied entry into such Rail Terminal for reasons attributable to the\n",
      "Concessionaire and for the Rail Terminal operator\n",
      "6.4\n",
      "Railway Administration's Liability for Delayed Supply\n",
      "6.4.1\n",
      "Railway Administration shall, in the event of delayed supply of locomotive beyond 12 hours\n",
      "from the indented schedule time o\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 39}\n",
      "move in rullway wagons in trainload as notified commodities, and such commodities may be\n",
      "subjected to different tariff and conditions for haulage. Provided further that the tariff so\n",
      "specified for notified commodities shall not exceed the freight rate as per the prevalent freight\n",
      "tariff schedule with chargeable weight, as for the trainload of railway wagons, generally used\n",
      "for their carriage.\n",
      "3.4\n",
      "Right to Encumber\n",
      "3.4.1\n",
      "The Patties hereby expressly acknowledge and agree that nothing in this Agreement shall\n",
      "prohibit or in any way preclude the ability or right of the Concessionaire to Encumber its\n",
      "interest in favour of its Lenders, any Rail Terminals owned by the Concessionaire or held by\n",
      "it on lease or licence, the Concessionaire's Wagons, this Agreement including but not limited\n",
      "to the Grant provided herounder or any other asset owned by the Concessionaire and forming\n",
      "part of Its rolling stock, provided that DO property which has been leased/licensed by Railway\n",
      "Administration to the Co\n",
      "================================================================================\n",
      "SOURCE: {'source': 'PPP_Agreement', 'chunk_id': 181}\n",
      "by operator to get undue benefit of telescopic rate, the hub will be approved and notified\n",
      "to the Railway Administration by the Railway Board.\n",
      "4.2 The Originating point will clearly indicate on the Railway Receipt (RR) whether the\n",
      "container will go directly to the destination or will move via A specified hub.\n",
      "4.3 The hanlage charges for the loaded container will be charged from the originating point to the\n",
      "destination point for the entire distance of actual haulage via the specified \"hub\".\n",
      "4.4 For the loaded/empty container transhipped at the \"hub\", a subsequent RR with \"zero\" freight\n",
      "will be prepared for the distance from the hub to the destination. duly cross-referring the\n",
      "original RR and the originating point on the subsequent RR.\n",
      "4.5 The RR for recovery of hanlage charges will be prepared by Railway Staff posted at\n",
      "ICD/port/railway terminal on the basis of summary given by the operator of ICD\n",
      "indicating weight of consignment including the tare weight of container.\n",
      "5.0\n",
      "Commodities r\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from utils.vecDb import LegalVectorDB, SECTION_QUERIES\n",
    "# =========================\n",
    "# EXAMPLE USAGE\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Example flow:\n",
    "    - Load document text\n",
    "    - Ingest\n",
    "    - Build index\n",
    "    - Retrieve section-wise chunks\n",
    "    \"\"\"\n",
    "\n",
    "    vecdb = LegalVectorDB()\n",
    "\n",
    "    # Example: load text (replace with PDF-to-text output)\n",
    "    with open(\"extracted_text/Adani  Mundra Port agreement for operations.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    vecdb.ingest_document(text, source_name=\"PPP_Agreement\")\n",
    "    vecdb.build_index()\n",
    "\n",
    "    # Retrieve for Section 6: Tariff & Revenue Flexibility\n",
    "    section_id = 6\n",
    "    query = SECTION_QUERIES[section_id]\n",
    "\n",
    "    retrieved_chunks = vecdb.retrieve_for_section(query, top_k=10)\n",
    "\n",
    "    for doc in retrieved_chunks:\n",
    "        print(\"SOURCE:\", doc.metadata)\n",
    "        print(doc.page_content[:1000])\n",
    "        print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc2191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Freight on Through Distance Basis issued on 24.09.2014 (Annexure “A” of the policy),\n",
      "as modified from time to time.\n",
      "13.2 All new GCTs charged on a through distance basis shall be\n",
      "governed by the Engine-on-Load policy (FM Circular No.5 of 2023 dated\n",
      "07.03.2013}, as modified from time to time.\n",
      "13.2.1 Railways may permit commissioning of a new GCT on non-EOL\n",
      "basis with the approval of DRM, If Engine-on-Load scheme Is not\n",
      "operationally feasible.\n",
      "13.2.2 A new GCT/existing terminal migrating to GCT policy is\n",
      "approved on non-through distance basis - then the maintenance of\n",
      "assets on non-Railway land (except OHE) - shall continue to be the\n",
      "responsibility of GCTO [as per provision of Para 7.3.4 of the GCT policy}.\n",
      "14. Charging of Commercial Staff\n",
      "14.1 No cost of commercial staff will be charged from the GCTO w.e.f.\n",
      "the date of issue of this policy. However, for existing Terminals (where\n",
      "PFT Private Siding Agreement has already entered into between AA and\n",
      "\n",
      "--- Result 2 ---\n",
      "the date of issue of this policy. However, for existing Terminals (where\n",
      "PFT Private Siding Agreement has already entered into between AA and\n",
      "Applicant before the issue of this Policy) where cost of commercial staff\n",
      "has already been deposited by the GCTO, there shall be no refund.\n",
      "Further, dues of commercial staff cost pending, if any, as on the date of\n",
      "issue of this policy shall be payable by GCTO.\n",
      "15. Provision of Weighbridge\n",
      "15.1 All ects Planning to deal with outward cargo (loading) shall\n",
      "suitable count of Electronic In-motion Weighbridge (EIMWB) at 2 SIML ereen Inside the Terminal,\n",
      "The EIMWB shall comply with the {available at the Organization of Legal Metrology} document\n",
      "65-//www.olmlongfensMes/mit rf ri06-2-e12.edf} and\n",
      "the latest ADSO specifications. Rw (Oat path\n",
      "14 Provision of EIMWE shall not be essential for GCTs dealing\n",
      "with inward cargo (unloading) only.\n",
      "15.1.2 If a GCT without an EIMWB plans to start loading, the\n",
      "\n",
      "--- Result 3 ---\n",
      "avoid any tampering with the system including software, The “admin” control should rest with the Railways.\n",
      "15.12.3 One-to-one correspondence between weight-o-meter\n",
      "discharge and corresponding wagon shall be maintained by GCTO.\n",
      "15.12.4 A procedure order for normal operations covering\n",
      "all the necessary steps to be taken to ensure that wagons are empty before\n",
      "loading and periodical test weighing of this system shall be\n",
      "maintained by GCTO,\n",
      "15.13 GCTO may also be permitted to Install an alternative means of\n",
      "weighment of wagons, which should have accuracy level at least equal to\n",
      "that of EIMWS and should ensure weighment within the time taken by\n",
      "EIMWE.\n",
      "15.13.1 GCTO shall submit requests for such alternative means\n",
      "of weighment, along with all the necessary documents, which may be\n",
      "permitted by Railway on case-to-case basis, on the joint\n",
      "recommendation of PCOM, PCME, PCE & POCM and with the personal\n",
      "approval of the General Manager.\n",
      "15.13.2 GCTO shall be responsible for getting the proposed\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the payment terms\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(res.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64fadc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with `$HOMEBREW_AUTO_UPDATE_SECS` or disable with\n",
      "`$HOMEBREW_NO_AUTO_UPDATE=1`. Hide these hints with `$HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n",
      "Installing from the API is now the default behaviour!\n",
      "You can save space and time by running:\n",
      "  brew untap homebrew/core\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 2 taps (homebrew/core and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "azure-dev: Developer CLI that provides commands for working with Azure resources\n",
      "libthai: Thai language support library\n",
      "pgroll: Postgres zero-downtime migrations made easy\n",
      "rig-r: R Installation Manager\n",
      "rv-r: Declarative R package manager\n",
      "shiki: Beautiful yet powerful syntax highlighter\n",
      "xcsift: Swift tool to parse xcodebuild output for coding agents\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "eigent: Desktop AI agent\n",
      "font-zxgamut\n",
      "hytale: Official Hytale Launcher\n",
      "stremioservice: Companion app for Stremio Web\n",
      "\n",
      "You have \u001b[1m7\u001b[0m outdated formulae and \u001b[1m1\u001b[0m outdated cask installed.\n",
      "\u001b[31mError:\u001b[0m Cask 'cmake' definition is invalid: 'conflicts_with' stanza failed with: Calling conflicts_with formula: is disabled! There is no replacement.\n",
      "\u001b[33mWarning:\u001b[0m No available formula with the name \"langchain-groq\". Did you mean langgraph-cli?\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSearching for similarly named formulae and casks...\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mFormulae\u001b[0m\n",
      "langgraph-cli\n",
      "\n",
      "To install langgraph-cli, run:\n",
      "  brew install langgraph-cli\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCasks\u001b[0m\n",
      "langgraph-studio\n",
      "\n",
      "To install langgraph-studio, run:\n",
      "  brew install --cask langgraph-studio\n"
     ]
    }
   ],
   "source": [
    "!brew install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d24d91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_groq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_groq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGroq \u001b[38;5;28;01mas\u001b[39;00m Groq\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_rag_qa_chain\u001b[39m(vector_store, api_key):\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_groq'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# from langchain_groq import ChatGroq as Groq\n",
    "from langchain import Groq\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def setup_rag_qa_chain(vector_store, api_key):\n",
    "    \"\"\"Setup RAG chain for Q&A\"\"\"\n",
    "    \n",
    "    llm = Groq(\n",
    "        api_key=api_key,\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"You are a legal expert analyzing a Master Concession Agreement.\n",
    "Use the provided context to answer the question accurately.\n",
    "If the answer is not in the context, say \"The information is not available in the document.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cag_env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
